{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "projet_ccf.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "z5vh4H9B_qUO"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uikGSgStzO6p",
    "outputId": "fea3901d-75de-48d7-89e3-24b63261fde0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 66.0 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=d54cfa3dc623034e9e0d3ddbf0f322c81157b682fcfb897292cc46a4f43838f5\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.accumulators import AddingAccumulatorParam\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time"
   ],
   "metadata": {
    "id": "wNw9Pc3NzuE1"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "acc = sc.accumulator(0)"
   ],
   "metadata": {
    "id": "IVDf1W3tzwIF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **CCF CODEBASE**"
   ],
   "metadata": {
    "id": "JNHw3zoak0wE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CCF:\n",
    "    def __init__(self, sc, graph_data, nodes=None):\n",
    "        self.sc = sc\n",
    "        if nodes != None:\n",
    "            self.raw_data = sc.parallelize(graph_data, nodes)\n",
    "        else:\n",
    "            self.raw_data = sc.parallelize(graph_data)\n",
    "        self.spark_nodes = nodes\n",
    "        self.components = None\n",
    "        self.vertices_length = len(graph_data[0][0])\n",
    "        self.step_time = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the self.components variable\n",
    "        \"\"\"\n",
    "        self.components = None\n",
    "\n",
    "    def find_connected_components(\n",
    "        self, secondary_sorting=False, debug_time=False, debug=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Finds connected components using CCF\n",
    "        \"\"\"\n",
    "        if self.components != None:\n",
    "            # avoid recalculating but uses space:\n",
    "            return self.components\n",
    "        else:\n",
    "            # keep raw_data untouched for future use, proceed with rdd variable:\n",
    "            rdd = self.raw_data\n",
    "            PAIR_CREATED = False\n",
    "            start = time.perf_counter()\n",
    "            iteration = 0\n",
    "            while True:\n",
    "                # new pair counter:\n",
    "                acc.value = 0\n",
    "                # iterating...\n",
    "                rdd = self.iterate(rdd, secondary_sorting, debug_time)\n",
    "                iteration += 1\n",
    "                PAIR_CREATED = bool(acc.value)\n",
    "                if debug:\n",
    "                    print(\"iterate {}:\".format(iteration), rdd.collect())\n",
    "                # testing terminating condition:\n",
    "                if PAIR_CREATED == False:\n",
    "                    break\n",
    "            end = time.perf_counter()\n",
    "            print(\"ccf exec. time: {}\".format(end - start))\n",
    "            # saving results:\n",
    "            self.components = rdd\n",
    "            return self.components\n",
    "\n",
    "    def iterate(self, rdd, secondary_sorting, debug_time):\n",
    "        \"\"\"\n",
    "        Iterates RDD according to the CCF process\n",
    "        \"\"\"\n",
    "        self.step_time = time.perf_counter()\n",
    "        # mapping job:\n",
    "        rdd = self.map(rdd)\n",
    "        self.print_time(debug_time, \"map\")\n",
    "        # shuffle & sort:\n",
    "        rdd = self.shuffle_and_sort(rdd, secondary_sorting)\n",
    "        self.print_time(debug_time, \"s&s\")\n",
    "        # reducing job:\n",
    "        rdd = self.reduce(rdd, secondary_sorting)\n",
    "        self.print_time(debug_time, \"reduce\")\n",
    "        # deduplicating job:\n",
    "        rdd = self.dedup(rdd)\n",
    "        self.print_time(debug_time, \"dedup\")\n",
    "        if debug_time:\n",
    "            print(\"---\")\n",
    "        return rdd\n",
    "\n",
    "    def map(self, rdd):\n",
    "        \"\"\"\n",
    "        CCF Map Job\n",
    "        \"\"\"\n",
    "        # flattening the newly created pairs (edges):\n",
    "        return rdd.flatMap(lambda x: [(x[1], x[0]), (x[0], x[1])])\n",
    "\n",
    "    def shuffle_and_sort(self, rdd, secondary_sorting):\n",
    "        \"\"\"\n",
    "        CCF Shuffle & Sort\n",
    "        \"\"\"\n",
    "        if secondary_sorting:\n",
    "            rdd = self.secondary_sort(rdd)\n",
    "        # grouping by key for the reducing job:\n",
    "        rdd = rdd.groupByKey()\n",
    "        return rdd\n",
    "\n",
    "    def reduce(self, rdd, secondary_sorting):\n",
    "        \"\"\"\n",
    "        CCF Reduce Job\n",
    "        \"\"\"\n",
    "\n",
    "        def reduce_pair(key_value, ss):\n",
    "            \"\"\"\n",
    "            Reduces (key, values) pair\n",
    "            \"\"\"\n",
    "            key = key_value[0]\n",
    "            values = key_value[1]\n",
    "            if ss:\n",
    "                it = values.__iter__()\n",
    "                minimum = next(it)\n",
    "                if minimum < key:\n",
    "                    yield (key, minimum)\n",
    "                    for value in it:\n",
    "                        acc.add(1)\n",
    "                        yield (value, minimum)\n",
    "            else:\n",
    "                value_list = []\n",
    "                minimum = key\n",
    "                for value in values:\n",
    "                    if value < minimum:\n",
    "                        minimum = value\n",
    "                    value_list.append(value)\n",
    "                if minimum < key:\n",
    "                    yield (key, minimum)\n",
    "                    for value in value_list:\n",
    "                        if value != minimum:\n",
    "                            acc.add(1)\n",
    "                            yield (value, minimum)\n",
    "\n",
    "        # flattening emitted pairs (edges):\n",
    "        rdd = rdd.flatMap(lambda x: list(reduce_pair(x, secondary_sorting)))\n",
    "        # (implementation purpose: triggers new pair counter collection)\n",
    "        rdd.foreach(lambda x: None)\n",
    "        return rdd\n",
    "\n",
    "    def secondary_sort(self, rdd):\n",
    "        \"\"\"\n",
    "        Secondary Sort\n",
    "        \"\"\"\n",
    "        # creating composite keys: (key+value, None):\n",
    "        rdd = rdd.map(lambda x: (x[0] + x[1], None))\n",
    "        # sorting pairs:\n",
    "        rdd = rdd.sortByKey()\n",
    "        # (avoid pyspark error)\n",
    "        length = self.vertices_length\n",
    "        # retransforming pairs: (key, value):\n",
    "        rdd = rdd.map(lambda x: (x[0][0 : 0 + length], x[0][length:]))\n",
    "        # collecting unique keys: (USELESS)\n",
    "        # keys = np.unique(rdd.keys().collect()).tolist()\n",
    "        # partitioning for future grouping: (USELESS)\n",
    "        # rdd = rdd.partitionBy(len(keys), lambda x: keys.index(x))\n",
    "        return rdd\n",
    "\n",
    "    def dedup(self, rdd):\n",
    "        \"\"\"\n",
    "        Removes pair's duplicates\n",
    "        \"\"\"\n",
    "        # transform (key,value) into (key+value, None) --> group by key --> keep only one occurence:\n",
    "        rdd = rdd.map(lambda x: (x, None)).groupByKey().map(lambda x: x[0])\n",
    "        return rdd\n",
    "\n",
    "    def print_time(self, debug_time, job):\n",
    "        \"\"\"\n",
    "        Displays jobs execution time\n",
    "        \"\"\"\n",
    "        if debug_time:\n",
    "            now = time.perf_counter()\n",
    "            job_time = now - self.step_time\n",
    "            print(\"- {}: {}\".format(job, job_time))\n",
    "            self.step_time = now"
   ],
   "metadata": {
    "id": "I2uFiWUAz4ys",
    "cellView": "code"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **PAPER EXAMPLE**"
   ],
   "metadata": {
    "id": "z5vh4H9B_qUO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "edges = [(\"A\", \"B\"), (\"B\", \"D\"), (\"B\", \"C\"), (\"D\", \"E\"), (\"F\", \"G\"), (\"G\", \"H\")]\n",
    "\n",
    "instance = CCF(sc, edges)\n",
    "res = instance.find_connected_components(debug_time=False, debug=False)\n",
    "print(\" - (Paper example) connected  comp.:\")\n",
    "print(res.collect())"
   ],
   "metadata": {
    "id": "3tuUan-F0IiT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eaba9d4a-6cd0-4f88-f03a-2a2dfd2dcc4d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ccf exec. time: 6.626811511999904\n",
      " - (Paper example) connected  comp.:\n",
      "[('B', 'A'), ('E', 'A'), ('D', 'A'), ('G', 'F'), ('H', 'F'), ('C', 'A')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "instance.reset()\n",
    "\n",
    "res = instance.find_connected_components(\n",
    "    debug_time=False, debug=False, secondary_sorting=True\n",
    ")\n",
    "print(\"- (Paper example) connected  comp. (secondary sorting):\")\n",
    "print(res.collect())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b58ItSjLypkC",
    "outputId": "0e2822a8-fb23-412d-8d61-840d133bf15d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ccf exec. time: 5.257217989999845\n",
      "- (Paper example) connected  comp. (secondary sorting):\n",
      "[('B', 'A'), ('D', 'A'), ('E', 'A'), ('G', 'F'), ('H', 'F'), ('C', 'A')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **TESTING: GOOGLE WEB GRAPH**"
   ],
   "metadata": {
    "id": "j2VRu-rXlLqH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class GoogleGraph:\n",
    "    def __init__(self):\n",
    "        self.edges = None\n",
    "        self.node_digits = 6\n",
    "\n",
    "    def import_graph(self):\n",
    "        edges = []\n",
    "        with open(\"drive/MyDrive/web-Google.txt\", \"r\") as reader:\n",
    "            line = reader.readline()\n",
    "            c = 0  # skip first lines\n",
    "            while line != \"\" and c < 10:\n",
    "                if c > 3:\n",
    "                    nodes = line.split(\"\\t\")\n",
    "                    edge = (\n",
    "                        nodes[0].zfill(self.node_digits),\n",
    "                        nodes[1].split(\"\\n\")[0].zfill(self.node_digits),\n",
    "                    )\n",
    "                    edges.append(edge)\n",
    "                else:\n",
    "                    c += 1\n",
    "                line = reader.readline()\n",
    "        self.edges = edges\n",
    "\n",
    "    def get_edges(self):\n",
    "        if self.edges != None:\n",
    "            return self.edges\n",
    "        else:\n",
    "            self.import_graph()\n",
    "            return"
   ],
   "metadata": {
    "id": "kNJg50Z0NWyn"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "g = GoogleGraph()\n",
    "g.import_graph()\n",
    "print(g.get_edges()[0:2])\n",
    "print(g.get_edges()[-2:])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTfnSBkE7MQu",
    "outputId": "9d788f92-9643-49e6-cd28-ccc09c7f20fd"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('000000', '011342'), ('000000', '824020')]\n",
      "[('916425', '637936'), ('916425', '837379')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "google_ccf = CCF(sc, g.get_edges())\n",
    "google_cc_std = google_ccf.find_connected_components(debug_time=True)\n",
    "print(\"#components:\", google_cc_std.map(lambda x: x[1]).distinct().count())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjKcV3FX71o5",
    "outputId": "8595428d-e886-4ac5-c76e-2f6dcd3f4e55"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "- map: 0.0003094399999099551\n",
      "- s&s: 0.046900127000071734\n",
      "- reduce: 79.34880173900001\n",
      "- dedup: 0.031098041999939596\n",
      "---\n",
      "- map: 1.8279999949299963e-05\n",
      "- s&s: 0.026213134000045102\n",
      "- reduce: 248.49539604300003\n",
      "- dedup: 0.03327128699993409\n",
      "---\n",
      "- map: 1.8245999854116235e-05\n",
      "- s&s: 0.026581432000057248\n",
      "- reduce: 131.3225960090001\n",
      "- dedup: 0.024229388999856383\n",
      "---\n",
      "- map: 2.5254999854951166e-05\n",
      "- s&s: 0.027879599000016242\n",
      "- reduce: 127.35053314699985\n",
      "- dedup: 0.02704241900028137\n",
      "---\n",
      "- map: 1.584699975865078e-05\n",
      "- s&s: 0.03419299300003331\n",
      "- reduce: 89.5147570830004\n",
      "- dedup: 0.03063217499993698\n",
      "---\n",
      "- map: 1.7675999970379053e-05\n",
      "- s&s: 0.020618737999939185\n",
      "- reduce: 56.73327731900008\n",
      "- dedup: 0.018731141999978718\n",
      "---\n",
      "- map: 1.5192999853752553e-05\n",
      "- s&s: 0.016867442000148003\n",
      "- reduce: 23.37400726499982\n",
      "- dedup: 0.020140273999913916\n",
      "---\n",
      "- map: 1.0429999747429974e-05\n",
      "- s&s: 0.02366459900031259\n",
      "- reduce: 22.987762718999875\n",
      "- dedup: 0.02287721099992268\n",
      "---\n",
      "- map: 1.005100011752802e-05\n",
      "- s&s: 0.021041093999883742\n",
      "- reduce: 22.649494644000242\n",
      "- dedup: 0.025155776999781665\n",
      "---\n",
      "ccf exec. time: 802.2639835840002\n",
      "#components: 2746\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "google_ccf.reset()\n",
    "google_cc_ss = google_ccf.find_connected_components(\n",
    "    debug_time=True, secondary_sorting=True\n",
    ")\n",
    "print(\"#components:\", google_cc_ss.map(lambda x: x[1]).distinct().count())"
   ],
   "metadata": {
    "id": "RzaMEz9W7n5B",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dbb32949-4c4b-445d-9bec-c714372a706f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "- map: 0.0007850850001887011\n",
      "- s&s: 18.579826112999854\n",
      "- reduce: 110.98940198599985\n",
      "- dedup: 0.041792764000092575\n",
      "---\n",
      "- map: 1.9083000097452896e-05\n",
      "- s&s: 253.22066570599964\n",
      "- reduce: 122.19920986500028\n",
      "- dedup: 0.020465346000037243\n",
      "---\n",
      "- map: 2.3858000076870667e-05\n",
      "- s&s: 153.74350883800025\n",
      "- reduce: 90.79504688499992\n",
      "- dedup: 0.02158926199990674\n",
      "---\n",
      "- map: 1.0443000064697117e-05\n",
      "- s&s: 155.09256084799972\n",
      "- reduce: 110.19486207599994\n",
      "- dedup: 0.021932164000190824\n",
      "---\n",
      "- map: 1.088699991669273e-05\n",
      "- s&s: 80.08770742200022\n",
      "- reduce: 45.427534124999966\n",
      "- dedup: 0.018323257999782072\n",
      "---\n",
      "- map: 1.2782999874616507e-05\n",
      "- s&s: 52.45926239900018\n",
      "- reduce: 32.423533187999965\n",
      "- dedup: 0.02185456699999122\n",
      "---\n",
      "- map: 1.4957000075810356e-05\n",
      "- s&s: 21.69100784500006\n",
      "- reduce: 28.219643306000307\n",
      "- dedup: 0.02079953599968576\n",
      "---\n",
      "- map: 1.0288999874319416e-05\n",
      "- s&s: 21.15454496600023\n",
      "- reduce: 16.860583767999742\n",
      "- dedup: 0.019400208000206476\n",
      "---\n",
      "- map: 1.1297000128251966e-05\n",
      "- s&s: 19.755967261000023\n",
      "- reduce: 27.720937835999848\n",
      "- dedup: 0.018159422999815433\n",
      "---\n",
      "ccf exec. time: 1360.8269937290002\n",
      "#components: 2746\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "graph = nx.Graph()\n",
    "for edge in g.get_edges():\n",
    "    graph.add_edge(edge[0], edge[1])\n",
    "print(\"#components (by networkx):\", nx.number_connected_components(graph))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkWBNgJDIClm",
    "outputId": "3c47af95-7d02-4845-fd1a-1a9a8d501056"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#components (by networkx): 2746\n"
     ]
    }
   ]
  }
 ]
}